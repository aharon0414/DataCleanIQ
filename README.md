# Data Quality Platform

> **AI-Native Data Cleaning & Quality Analysis Tool**  
> Combining Power BI-caliber visualizations with intelligent data preparation workflows

[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)
[![React](https://img.shields.io/badge/React-18-61dafb.svg)](https://reactjs.org/)
[![TailwindCSS](https://img.shields.io/badge/Tailwind-3.0-38bdf8.svg)](https://tailwindcss.com/)

**Portfolio Project by Aaron Sperry**  
AI-Native Developer & Business Intelligence Specialist

---

## ğŸ¯ Project Overview

This application automates data quality analysis and cleaning workflows, solving the critical first step of every analytics project: ensuring clean, reliable data **before** it reaches Power BI, Tableau, or any analytics tool.

**Core Problem:** Data analysts spend 60-80% of their time cleaning messy data. This tool detects quality issues, suggests intelligent fixes, and visualizes metrics with Power BI-caliber dashboardsâ€”achieving 49% quality improvement on real-world datasets while maintaining data integrity.

**Key Differentiators:**
- âœ… **Power BI-Quality Visualizations:** Interactive charts, heatmaps, and dashboards
- âœ… **AI-Powered Suggestions:** Claude API integration for intelligent cleaning recommendations
- âœ… **Visual Pipeline Builder:** Drag-and-drop cleaning workflows with live previews
- âœ… **Real-Time Analysis:** Instant quality scoring and issue detection
- âœ… **Export Flexibility:** Download cleaned data or generate Python/SQL scripts

---

## ğŸ¯ Design Philosophy: Safe Automation

DataCleanIQ follows a conservative, integrity-first approach to data transformation.

### âœ… Automated Fixes (Applied Safely)

The tool automatically suggests fixes for issues where automation is statistically valid and preserves data integrity:

**Duplicate Removal**
- Detects exact row matches (100% identical across all columns)
- Safe to remove automatically - duplicates add no information
- Preserves one instance of each unique row

**Numeric Column Fills**
- Missing values in numeric columns (quantity, price, revenue, etc.)
- Filled with statistical median - a reasonable approximation
- Example: Missing quantity â†’ filled with median of 3 units
- Mathematically sound and maintains data distribution

**Outlier Removal**
- Statistical outliers detected using IQR (Interquartile Range) method
- Values beyond 1.5 Ã— IQR from Q1/Q3 quartiles
- Example: Quantity of 9,999 units removed when typical range is 1-10
- Prevents extreme values from skewing analysis

### âš ï¸ Flagged Issues (Manual Review Required)

The tool identifies but does NOT auto-fill these issue types:

**Text and Identifier Columns**
- Email addresses, phone numbers, customer names, IDs
- Cannot be safely auto-filled with statistical methods
- Filling with "most common value" would corrupt data
- Tool flags the issue, preserves data integrity

**Example of why this matters:**
âŒ WRONG: Fill missing email with mode (most common)
Result: 3 different customers now share "john@email.com"
Impact: Email campaigns reach wrong people, data corruption
âœ… RIGHT: Flag the issue, let analyst decide
Options: Remove rows, use placeholder, implement custom logic
Impact: Data integrity maintained, informed decisions

### ğŸ Python Script Generation (Roadmap)

For complex transformations requiring business logic, the tool will generate Python/pandas scripts:
```python
# Custom email filling logic (coming soon)
import pandas as pd

df = pd.read_csv('your_data.csv')

# Business rule: Fill based on region
df.loc[df['region'] == 'CA', 'email'] = df['email'].fillna('noreply-ca@company.com')
df.loc[df['region'] == 'NY', 'email'] = df['email'].fillna('noreply-ny@company.com')

# Or construct from customer name
df['email'] = df.apply(
    lambda row: f"{row['customer_name'].lower().replace(' ', '.')}@company.com"
    if pd.isna(row['email']) else row['email'],
    axis=1
)

df.to_csv('cleaned_data.csv', index=False)
```

### ğŸ’¡ Core Principle

**Data Integrity > Completeness**

A dataset with honest gaps is more valuable than one with fabricated data. The tool prioritizes:
1. **Accuracy** - Only fill when statistically valid
2. **Transparency** - Flag issues that require human judgment  
3. **Trust** - Never silently corrupt data to appear "clean"
4. **Control** - Give analysts the tools to make informed decisions

This design makes DataCleanIQ suitable for production data pipelines where integrity matters.

---

## ğŸ“‹ Documentation

- **[Product Requirements](./REQUIREMENTS.md)** - Full feature specifications, user stories, and technical architecture
- **[API Documentation](#)** - Coming in Phase 2
- **[Contributing Guide](#)** - Coming soon

---

## ğŸ—ï¸ Technical Architecture

### Frontend Stack
```
React 18 + TypeScript
â”œâ”€â”€ UI Framework: TailwindCSS + shadcn/ui
â”œâ”€â”€ Data Grid: TanStack Table v8
â”œâ”€â”€ Visualizations: Recharts + D3.js
â”œâ”€â”€ File Parsing: PapaParse (CSV), SheetJS (Excel)
â””â”€â”€ Build Tool: Vite
```

### Backend Stack (Phase 2)
```
Node.js 20 + Express + TypeScript
â”œâ”€â”€ Database: PostgreSQL 15+
â”œâ”€â”€ ORM: Prisma (for pipeline storage)
â”œâ”€â”€ AI Integration: Anthropic Claude API
â””â”€â”€ Data Processing: Pandas (Python microservice, optional)
```

### Project Structure
```
data-quality-platform/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # React components
â”‚   â”‚   â”œâ”€â”€ ui/              # shadcn/ui primitives
â”‚   â”‚   â”œâ”€â”€ visualizations/  # Chart components (Recharts/D3)
â”‚   â”‚   â”œâ”€â”€ data-grid/       # TanStack Table implementation
â”‚   â”‚   â”œâ”€â”€ pipeline/        # Pipeline builder components
â”‚   â”‚   â””â”€â”€ ai-assistant/    # Claude chat interface
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ analysis/        # Data quality analysis engine
â”‚   â”‚   â”œâ”€â”€ transformations/ # Data cleaning logic
â”‚   â”‚   â”œâ”€â”€ scoring/         # Quality score algorithms
â”‚   â”‚   â””â”€â”€ utils/           # Shared utilities
â”‚   â”œâ”€â”€ hooks/               # Custom React hooks
â”‚   â”œâ”€â”€ types/               # TypeScript type definitions
â”‚   â”œâ”€â”€ stores/              # State management (Zustand)
â”‚   â””â”€â”€ api/                 # Backend API clients (Phase 2)
â”œâ”€â”€ server/                  # Backend code (Phase 2)
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ public/                  # Static assets
â””â”€â”€ tests/                   # Unit and integration tests
```

---

## ğŸš€ Getting Started

### Prerequisites
- **Node.js:** 20.x or higher
- **npm:** 10.x or higher
- **Git:** Latest version
- **Anthropic API Key:** Required for AI features (Phase 2)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/data-quality-platform.git
cd data-quality-platform

# Install dependencies
npm install

# Start development server
npm run dev
```

The app will open at `http://localhost:5173`

## ğŸ¯ Current Status (January 26, 2026)

### âœ… Completed - Interview Ready (MVP v1.0)

**Core Functionality:**
- **CSV File Upload** - Drag-and-drop with PapaParse integration âœ…
- **Quality Analysis Engine** - Detects missing values, outliers, duplicates âœ…
- **Power BI-Style Dashboard** - 4 interactive Recharts visualizations (gauge, bar, donuts) âœ…
- **Column Profiles** - Individual column statistics and quality scores âœ…
- **Smart Transformation Engine** - Conservative, integrity-first approach âœ…
  - âœ… Remove duplicates (exact matches)
  - âœ… Fill numeric columns with median (handles "N/A", "null", "na" string variants)
  - âœ… Remove outliers (IQR method)
  - âš ï¸ Flag text/identifier issues (no auto-fill to preserve integrity)
- **Auto-Suggested Fixes** - Intelligent suggestions based on detected issues âœ…
- **Export Cleaned Data** - Download as CSV âœ…
- **Detailed Logging** - Console instrumentation for transparency âœ…
- **File Metadata Display** - Name, size, row/column counts, timestamp âœ…

**Verified Performance:**
- **Test Dataset:** 10,000 rows with 30% missing values, 500 duplicates
- **Initial Score:** 43/100 (Poor quality)
- **After Cleaning:** 64/100 (Fair quality)
- **Improvement:** 49% quality increase in <5 seconds
- **Processing:** Handles 10k rows smoothly, fills 3,000+ values correctly

**Key Achievement:**
Successfully handles mixed-type data (numeric values with "N/A", "null", "na" string variants) by intelligently detecting column types and filling only statistically valid transformations.

### ğŸ¯ Design Decisions

**Conservative Fill Strategy:**
- Auto-fills ONLY numeric columns where median is statistically valid
- Detects and handles string variants of missing values ("N/A", "null", "na", "#N/A")
- Flags identifier columns (email, phone, names) rather than corrupt with fake data
- Transparent operations with detailed console logging
- Verified accuracy with controlled test datasets

**Product Positioning:**
Inspector/Recommender hybrid - Detects issues intelligently, suggests safe fixes, gives analyst control over transformations requiring domain knowledge.

### ğŸ“‹ Next Release (v1.1 - Post-Interview)

**Planned Features:**
- **Python Script Generation** - Export pandas code for custom transformations
- **SQL Script Generation** - Generate SQL cleaning scripts
- **Options Dropdown** - Choose fill strategy per transformation (median/mean/mode/0/remove)
- **Transformation Templates** - Pre-built workflows for common scenarios
- **Before/After Preview** - Modal showing impact before applying
- **Advanced Outlier Options** - Configurable IQR threshold

### ğŸ“Š Known Limitations

**Current:**
- Client-side processing only (100MB file limit)
- No backend/database (stateless)
- CSV format only
- Median-only fill strategy (no user choice)
- Text columns flagged but not auto-filled

**Accepted Tradeoffs:**
- Data integrity prioritized over completeness
- Conservative automation to prevent data corruption
- Manual review required for identifier columns
- No dependency detection (revenue â‰  quantity Ã— price after fills)

These limitations are documented and will be addressed in future versions via Python/SQL script generation for complex business rules.

### ğŸ¯ Interview Demo Ready

**Demonstrates:**
- âœ… Full-stack development (React + TypeScript)
- âœ… BI visualization expertise (Power BI-quality dashboards)
- âœ… Data quality domain knowledge (understands integrity vs. completeness tradeoffs)
- âœ… Product thinking (conservative by design, roadmap for expansion)
- âœ… AI-native development (built with Claude + Cursor)
- âœ… Technical leadership (systematic debugging, architectural decisions)

**Live Demo Available:** localhost:5173
**Portfolio:** https://github.com/yourusername/data-quality-platform

### Environment Variables

Create a `.env` file in the root directory:

```env
# Phase 1 (MVP) - No backend yet
VITE_APP_NAME=Data Quality Platform
VITE_APP_VERSION=1.0.0

# Phase 2+ (Backend & AI)
VITE_API_URL=http://localhost:3001
ANTHROPIC_API_KEY=your_api_key_here
DATABASE_URL=postgresql://user:password@localhost:5432/data_quality
JWT_SECRET=your_jwt_secret_here
```

**Important:** Never commit `.env` to version control. Add to `.gitignore`.

---

## ğŸ’» Development Workflow

### Running the App

```bash
# Development mode with hot reload
npm run dev

# Production build
npm run build

# Preview production build
npm run preview

# Type checking
npm run type-check

# Linting
npm run lint

# Format code
npm run format
```

### Testing

```bash
# Run all tests
npm test

# Run tests in watch mode
npm run test:watch

# Run tests with coverage
npm run test:coverage

# E2E tests (Playwright)
npm run test:e2e
```

### Code Quality Tools

**ESLint** - JavaScript/TypeScript linting
```bash
npm run lint
npm run lint:fix  # Auto-fix issues
```

**Prettier** - Code formatting
```bash
npm run format
npm run format:check  # Check without writing
```

**TypeScript** - Static type checking
```bash
npm run type-check
```

### Git Workflow

1. **Create feature branch:**
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. **Make changes and commit:**
   ```bash
   git add .
   git commit -m "feat: add your feature description"
   ```

3. **Keep branch updated:**
   ```bash
   git checkout main
   git pull origin main
   git checkout feature/your-feature-name
   git rebase main
   ```

4. **Push and create PR:**
   ```bash
   git push origin feature/your-feature-name
   # Then create Pull Request on GitHub
   ```

**Commit Message Convention:**
- `feat:` New feature
- `fix:` Bug fix
- `docs:` Documentation changes
- `style:` Code style changes (formatting)
- `refactor:` Code refactoring
- `test:` Adding or updating tests
- `chore:` Maintenance tasks

---

## ğŸ“Š Data Quality Analysis Engine

### Core Components

**1. Quality Analyzer**
```typescript
// src/lib/analysis/quality-analyzer.ts
export interface QualityReport {
  overallScore: number;
  columnScores: ColumnScore[];
  issues: Issue[];
  summary: QualitySummary;
}

export function analyzeDataQuality(data: any[]): QualityReport {
  // Detect missing values, duplicates, outliers, type issues
  // Calculate scores using algorithm from REQUIREMENTS.md
  // Return comprehensive report
}
```

**2. Issue Detectors**
```typescript
// src/lib/analysis/detectors/
â”œâ”€â”€ missing-values.ts      // Detect nulls, empty strings, "N/A", etc.
â”œâ”€â”€ duplicates.ts          // Exact and fuzzy duplicate detection
â”œâ”€â”€ outliers.ts            // Statistical outlier detection (IQR, Z-score)
â”œâ”€â”€ type-consistency.ts    // Data type validation and inference
â””â”€â”€ format-issues.ts       // Date formats, text casing, whitespace
```

**3. Scoring Algorithm**
```typescript
// Column Score = 100 - penalties
const columnScore = 100 
  - (missingPercent * 2)           // -2 points per 1% missing
  - (outlierPercent * 1)           // -1 point per 1% outliers
  - (typeInconsistencies * 5)      // -5 points per instance
  - (formatIssuePercent * 0.2)     // -1 point per 5% format issues
  - (duplicatePercent * 1.5);      // -1.5 points per 1% duplicates

// Floor at 0
return Math.max(0, columnScore);
```

### Transformation Engine

**4. Cleaning Operations**
```typescript
// src/lib/transformations/cleaners/
â”œâ”€â”€ fill-missing.ts        // Fill with constant, mean, median, forward-fill
â”œâ”€â”€ remove-duplicates.ts   // Remove exact or fuzzy duplicates
â”œâ”€â”€ filter-rows.ts         // Conditional filtering
â”œâ”€â”€ convert-types.ts       // Type coercion and validation
â”œâ”€â”€ text-transforms.ts     // Trim, case changes, regex replace
â””â”€â”€ numeric-transforms.ts  // Round, scale, absolute value
```

**5. Pipeline Execution**
```typescript
// src/lib/transformations/pipeline.ts
export interface CleaningStep {
  id: string;
  type: TransformationType;
  config: any;
  enabled: boolean;
}

export function executePipeline(
  data: any[],
  steps: CleaningStep[]
): { cleanedData: any[]; appliedSteps: string[] } {
  // Apply transformations sequentially
  // Maintain immutability (no data mutation)
  // Return cleaned data + audit trail
}
```

---

## ğŸ“ˆ Visualization Components

### Chart Library Usage

**Recharts (Primary)**
Use for standard charts: bar, line, pie, area, scatter
```tsx
import { BarChart, Bar, XAxis, YAxis, Tooltip, ResponsiveContainer } from 'recharts';

<ResponsiveContainer width="100%" height={300}>
  <BarChart data={columnScores}>
    <XAxis dataKey="column" />
    <YAxis domain={[0, 100]} />
    <Tooltip />
    <Bar dataKey="score" fill="#0078D4" />
  </BarChart>
</ResponsiveContainer>
```

**D3.js (Complex Visualizations)**
Use for heatmaps, correlation matrices, custom visualizations
```tsx
import * as d3 from 'd3';

// Heatmap example
const heatmap = d3.select(svgRef.current)
  .selectAll('rect')
  .data(matrixData)
  .enter()
  .append('rect')
  .attr('x', d => xScale(d.column))
  .attr('y', d => yScale(d.row))
  .attr('fill', d => colorScale(d.value));
```

### Key Visualizations

**Quality Dashboard**
```tsx
// src/components/visualizations/QualityDashboard.tsx
- Overall Score Gauge (Recharts RadialBarChart)
- Column Scores Bar Chart (Recharts BarChart)
- Issue Distribution Donut (Recharts PieChart)
- Data Completeness Heatmap (D3.js)
```

**Column Profile**
```tsx
// src/components/visualizations/ColumnProfile.tsx
- Distribution Histogram (Recharts BarChart)
- Missing Pattern Sparkline (Recharts LineChart)
- Top Values Bar Chart (Recharts BarChart)
```

**Correlation Matrix**
```tsx
// src/components/visualizations/CorrelationMatrix.tsx
- Heatmap showing issue correlations (D3.js)
```

---

## ğŸ¤– AI Integration (Phase 2)

### Claude API Setup

**Installation**
```bash
npm install @anthropic-ai/sdk
```

**Client Configuration**
```typescript
// src/lib/ai/claude-client.ts
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

export async function generateCleaningSuggestions(
  qualityReport: QualityReport
): Promise<CleaningSuggestion[]> {
  const response = await client.messages.create({
    model: 'claude-sonnet-4-20250514',
    max_tokens: 1000,
    messages: [{
      role: 'user',
      content: `Analyze this data quality report and suggest 3-5 cleaning steps:
      
${JSON.stringify(qualityReport, null, 2)}

Provide actionable suggestions with reasoning.`
    }]
  });
  
  // Parse response and return structured suggestions
}
```

### Natural Language Query Processing

**User Input:** "Remove rows where age is greater than 150"

**AI Response:**
```json
{
  "intent": "filter_rows",
  "column": "age",
  "operator": "<=",
  "value": 150,
  "preview": "Will remove 3 rows where age > 150"
}
```

**Implementation:**
```typescript
// src/lib/ai/query-processor.ts
export async function processNaturalLanguageQuery(
  query: string,
  columns: string[]
): Promise<TransformationConfig> {
  // Send query to Claude with column context
  // Parse structured response
  // Return transformation configuration
}
```

---

## ğŸ¨ UI/UX Guidelines

### Design System

**Color Palette**
```css
/* Primary Colors */
--primary: #0078D4;        /* Microsoft Blue */
--success: #107C10;        /* Green */
--warning: #FF8C00;        /* Orange */
--error: #D13438;          /* Red */

/* Neutral Colors */
--gray-900: #323130;       /* Dark text */
--gray-700: #605E5C;       /* Medium text */
--gray-300: #EDEBE9;       /* Borders */
--gray-100: #F3F2F1;       /* Backgrounds */

/* Data Visualization */
--viz-1: #0078D4;
--viz-2: #00BCF2;
--viz-3: #00B294;
--viz-4: #7FBA00;
--viz-5: #FFB900;
```

**Typography**
```css
/* Font Family */
font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui;

/* Headings */
h1: 32px / 600 weight
h2: 24px / 600 weight
h3: 20px / 600 weight

/* Body */
body: 16px / 400 weight
small: 14px / 400 weight

/* Code */
code: 'JetBrains Mono', monospace
```

**Spacing Scale** (Tailwind)
```
0.5 = 2px    (space-0.5)
1   = 4px    (space-1)
2   = 8px    (space-2)
4   = 16px   (space-4)
6   = 24px   (space-6)
8   = 32px   (space-8)
```

### Component Library (shadcn/ui)

Install components as needed:
```bash
npx shadcn-ui@latest add button
npx shadcn-ui@latest add card
npx shadcn-ui@latest add dialog
npx shadcn-ui@latest add table
```

**Key Components:**
- `Button` - Primary actions
- `Card` - Content containers
- `Table` - Data grid wrapper
- `Dialog` - Modals and overlays
- `Select` - Dropdowns
- `Slider` - Range inputs
- `Tabs` - Navigation
- `Badge` - Status indicators

---

## ğŸ§ª Testing Strategy

### Unit Tests (Vitest)

**Example: Quality Analyzer**
```typescript
// tests/lib/analysis/quality-analyzer.test.ts
import { describe, it, expect } from 'vitest';
import { analyzeDataQuality } from '@/lib/analysis/quality-analyzer';

describe('Quality Analyzer', () => {
  it('should detect missing values', () => {
    const data = [
      { name: 'Alice', age: 30 },
      { name: 'Bob', age: null },
      { name: '', age: 25 }
    ];
    
    const report = analyzeDataQuality(data);
    
    expect(report.issues).toContainEqual({
      type: 'missing_values',
      column: 'age',
      count: 1,
      percentage: 33.33
    });
  });
  
  it('should calculate correct quality scores', () => {
    // Test scoring algorithm
  });
});
```

### Integration Tests

**Example: Pipeline Execution**
```typescript
// tests/integration/pipeline.test.ts
it('should apply multiple transformations in sequence', () => {
  const pipeline = [
    { type: 'fill_missing', config: { column: 'age', value: 0 } },
    { type: 'remove_duplicates', config: {} },
    { type: 'filter_rows', config: { column: 'age', operator: '>', value: 0 } }
  ];
  
  const result = executePipeline(messyData, pipeline);
  
  expect(result.cleanedData).toHaveLength(8);
  expect(result.appliedSteps).toHaveLength(3);
});
```

### E2E Tests (Playwright)

**Example: User Workflow**
```typescript
// tests/e2e/upload-and-clean.spec.ts
import { test, expect } from '@playwright/test';

test('complete data cleaning workflow', async ({ page }) => {
  await page.goto('/');
  
  // Upload file
  await page.setInputFiles('input[type="file"]', 'tests/fixtures/messy-data.csv');
  
  // Wait for analysis
  await expect(page.locator('.quality-score')).toBeVisible();
  
  // Apply suggested fix
  await page.click('button:has-text("Apply Suggested Fixes")');
  
  // Export cleaned data
  await page.click('button:has-text("Export Cleaned Data")');
  
  // Verify download
  const download = await page.waitForEvent('download');
  expect(download.suggestedFilename()).toContain('_cleaned.csv');
});
```

---

## ğŸš¢ Deployment

### Environment Setup

**Frontend (Vercel)**
```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel --prod
```

**Backend (Railway) - Phase 2**
```bash
# Install Railway CLI
npm i -g @railway/cli

# Login and deploy
railway login
railway up
```

### Environment Variables (Production)

Set in Vercel/Railway dashboard:
```
VITE_API_URL=https://your-backend.up.railway.app
ANTHROPIC_API_KEY=sk-ant-xxx
DATABASE_URL=postgresql://...
NODE_ENV=production
```

### Performance Optimization

**Code Splitting**
```typescript
// Lazy load heavy components
const PipelineBuilder = lazy(() => import('./components/pipeline/PipelineBuilder'));
```

**Bundle Analysis**
```bash
npm run build
npx vite-bundle-visualizer
```

**Lighthouse Score Targets:**
- Performance: >90
- Accessibility: >95
- Best Practices: >90
- SEO: >90

---

## ğŸ“¦ Key Dependencies

### Production Dependencies
```json
{
  "react": "^18.2.0",
  "react-dom": "^18.2.0",
  "typescript": "^5.0.0",
  "@tanstack/react-table": "^8.0.0",
  "recharts": "^2.10.0",
  "d3": "^7.9.0",
  "papaparse": "^5.4.1",
  "xlsx": "^0.18.5",
  "zustand": "^4.5.0",
  "date-fns": "^3.0.0",
  "lodash": "^4.17.21",
  "@anthropic-ai/sdk": "^0.30.0"  // Phase 2
}
```

### Development Dependencies
```json
{
  "vite": "^5.0.0",
  "vitest": "^1.0.0",
  "@playwright/test": "^1.45.0",
  "eslint": "^8.0.0",
  "prettier": "^3.0.0",
  "@types/react": "^18.2.0",
  "@types/lodash": "^4.17.0",
  "tailwindcss": "^3.4.0"
}
```

---

## ğŸ¤ Contributing

### Development Process

1. **Check REQUIREMENTS.md** for feature specifications
2. **Create feature branch** from `main`
3. **Write tests first** (TDD approach)
4. **Implement feature** following code style
5. **Update documentation** if needed
6. **Submit PR** with description

### Code Style

**TypeScript:**
- Use TypeScript for all new code
- Avoid `any` type; use proper types
- Export interfaces for public APIs
- Use descriptive variable names

**React:**
- Functional components with hooks
- Props destructuring
- Named exports for components
- Custom hooks for shared logic

**Formatting:**
- 2-space indentation
- Single quotes for strings
- Trailing commas
- Semicolons required

---

## ğŸ› Troubleshooting

### Common Issues

**Issue:** "Module not found: papaparse"
```bash
npm install papaparse @types/papaparse
```

**Issue:** Charts not rendering
```bash
# Ensure Recharts is installed
npm install recharts

# Check that ResponsiveContainer has width/height
<ResponsiveContainer width="100%" height={300}>
```

**Issue:** TypeScript errors in D3
```bash
npm install @types/d3
```

**Issue:** Slow performance with large files
```typescript
// Use React.memo for expensive components
export const DataTable = React.memo(({ data }) => {
  // Component logic
});

// Use useMemo for expensive calculations
const qualityReport = useMemo(() => analyzeDataQuality(data), [data]);
```

---

## ğŸ“š Learning Resources

### React + TypeScript
- [React Documentation](https://react.dev/)
- [TypeScript Handbook](https://www.typescriptlang.org/docs/)
- [TanStack Table Docs](https://tanstack.com/table/v8)

### Data Visualization
- [Recharts Documentation](https://recharts.org/)
- [D3.js Gallery](https://d3-graph-gallery.com/)
- [Observable HQ](https://observablehq.com/) - D3 examples

### AI Integration
- [Anthropic API Docs](https://docs.anthropic.com/)
- [Claude Cookbook](https://github.com/anthropics/anthropic-cookbook)

### TailwindCSS
- [Tailwind Documentation](https://tailwindcss.com/docs)
- [shadcn/ui Components](https://ui.shadcn.com/)

---

## ğŸ“ˆ Roadmap

### Phase 1: MVP âœ… (Current)
- âœ… CSV upload and parsing
- âœ… Basic data table display
- ğŸ”„ Quality analysis engine
- ğŸ”„ Visualization dashboard
- ğŸ“‹ Cleaning workflows
- ğŸ“‹ Export functionality

### Phase 2: Enhanced (Next)
- ğŸ“‹ PostgreSQL connection
- ğŸ“‹ Claude API integration
- ğŸ“‹ AI chat assistant
- ğŸ“‹ Advanced visualizations

### Phase 3: Advanced (Future)
- ğŸ“‹ Google Sheets integration
- ğŸ“‹ ML-based PII detection
- ğŸ“‹ Scheduled pipelines
- ğŸ“‹ Collaboration features

**Legend:** âœ… Complete | ğŸ”„ In Progress | ğŸ“‹ Planned

---

## ğŸ“„ License

MIT License - Feel free to use this project for learning and portfolio purposes.

---

## ğŸ‘¤ Author

**Aaron Sperry**  
AI-Native Developer & Business Intelligence Specialist

- LinkedIn: [aaron-sperry](https://linkedin.com/in/aaron-sperry)
- GitHub: [@aaronsperry](https://github.com/aaronsperry)
- Portfolio: [aaronsperry.dev](https://aaronsperry.dev)

---

## ğŸ™ Acknowledgments

- **Claude (Anthropic)** - AI pair programming assistant
- **Cursor** - AI-powered IDE
- **shadcn/ui** - Component library
- **TanStack** - Excellent table library
- **Recharts** - Easy-to-use charting library

---

**Last Updated:** January 26, 2026  
**Version:** 1.0.0 (Interview Ready)  
**Status:** âœ… Production POC - 49% quality improvement demonstrated on 10k dataset
